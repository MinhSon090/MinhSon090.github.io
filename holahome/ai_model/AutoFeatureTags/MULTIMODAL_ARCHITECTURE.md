# ğŸ–¼ï¸ MÃ´ HÃ¬nh Multimodal: Text + Image Feature Extraction

## ğŸ“Š So SÃ¡nh Kiáº¿n TrÃºc

### **MÃ´ HÃ¬nh Hiá»‡n Táº¡i (Text-only)**
```
Text Input â†’ PhoBERT â†’ Classification Head â†’ Features
```

### **MÃ´ HÃ¬nh Má»Ÿ Rá»™ng (Text + Image)**
```
Text Input  â”€â”€â”€â”€â”
                â”œâ”€â”€â–º Fusion â†’ Classification â†’ Features
Image Input â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Kiáº¿n TrÃºc Multimodal vá»›i UNet + Attention

### **1. Pipeline Tá»•ng Quan**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MULTIMODAL FEATURE EXTRACTION PIPELINE              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Text Branch:                          Image Branch:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "PhÃ²ng mÃ¡t,    â”‚                   â”‚  [Room Photo]   â”‚
â”‚   cÃ³ Ä‘iá»u hÃ²a"  â”‚                   â”‚   512Ã—512Ã—3     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                     â”‚
        â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PhoBERT       â”‚                   â”‚   UNet Encoder  â”‚
â”‚   Encoder       â”‚                   â”‚   (ResNet34)    â”‚
â”‚   768-dim       â”‚                   â”‚   512-dim       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                     â”‚
        â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚              â”‚
        â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CROSS-MODAL ATTENTION          â”‚
â”‚   (Text attends to Image)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FUSION LAYER                   â”‚
â”‚   Concat [768 + 512] â†’ 1280      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLASSIFICATION HEAD            â”‚
â”‚   Dense(1280 â†’ 100)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
    Output Features
```

---

## ğŸ¨ UNet cho Image Encoding

### **Táº¡i sao dÃ¹ng UNet?**

UNet vá»‘n Ä‘Æ°á»£c thiáº¿t káº¿ cho **segmentation**, nhÆ°ng pháº§n **Encoder** ráº¥t tá»‘t cho:
- âœ… TrÃ­ch xuáº¥t multi-scale features (low â†’ high level)
- âœ… Giá»¯ Ä‘Æ°á»£c spatial information (vá»‹ trÃ­ cÃ¡c Ä‘á»‘i tÆ°á»£ng)
- âœ… Lightweight hÆ¡n ViT (Vision Transformer)

### **Kiáº¿n TrÃºc UNet Encoder**

```
Input Image (512Ã—512Ã—3)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv Block 1                    â”‚
â”‚  64 channels (512Ã—512)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ MaxPool â†“
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv Block 2                    â”‚
â”‚  128 channels (256Ã—256)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ MaxPool â†“
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv Block 3                    â”‚
â”‚  256 channels (128Ã—128)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ MaxPool â†“
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Conv Block 4                    â”‚
â”‚  512 channels (64Ã—64)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ MaxPool â†“
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bottleneck                      â”‚
â”‚  512 channels (32Ã—32)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ Global Average Pooling
    â–¼
  512-dim vector
```

**Code Ä‘Æ¡n giáº£n:**
```python
class UNetEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        # Sá»­ dá»¥ng ResNet34 lÃ m backbone
        resnet = torchvision.models.resnet34(pretrained=True)
        
        # Láº¥y cÃ¡c layer tá»« ResNet
        self.conv1 = resnet.conv1
        self.bn1 = resnet.bn1
        self.relu = resnet.relu
        self.maxpool = resnet.maxpool
        
        self.layer1 = resnet.layer1  # 64 channels
        self.layer2 = resnet.layer2  # 128 channels
        self.layer3 = resnet.layer3  # 256 channels
        self.layer4 = resnet.layer4  # 512 channels
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    
    def forward(self, x):
        # Input: [batch, 3, 512, 512]
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        x = self.layer1(x)  # [batch, 64, 128, 128]
        x = self.layer2(x)  # [batch, 128, 64, 64]
        x = self.layer3(x)  # [batch, 256, 32, 32]
        x = self.layer4(x)  # [batch, 512, 16, 16]
        
        x = self.avgpool(x)  # [batch, 512, 1, 1]
        x = x.view(x.size(0), -1)  # [batch, 512]
        
        return x
```

---

## ğŸ”— Cross-Modal Attention

### **Táº¡i sao cáº§n Attention?**

Attention giÃºp:
- âœ… Text focus vÃ o cÃ¡c vÃ¹ng quan trá»ng cá»§a áº£nh
- âœ… áº¢nh cung cáº¥p context cho text
- âœ… Há»c Ä‘Æ°á»£c má»‘i quan há»‡ giá»¯a tá»« vÃ  visual features

### **Kiáº¿n TrÃºc Attention**

```
Text Features (Q)     Image Features (K, V)
     [768]                   [512]
       â”‚                       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚         â”‚             â”‚
       â–¼         â–¼             â–¼
   Query (Q)  Key (K)      Value (V)
   [768â†’512]  [512â†’512]   [512â†’512]
       â”‚         â”‚             â”‚
       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
     Attention(Q,K,V) = softmax(QÂ·K^T/âˆšd)Â·V
            â”‚
            â–¼
    Attended Features [512]
```

**Code Ä‘Æ¡n giáº£n:**
```python
class CrossModalAttention(nn.Module):
    def __init__(self, text_dim=768, image_dim=512):
        super().__init__()
        self.query = nn.Linear(text_dim, image_dim)
        self.key = nn.Linear(image_dim, image_dim)
        self.value = nn.Linear(image_dim, image_dim)
        self.scale = image_dim ** 0.5
    
    def forward(self, text_features, image_features):
        # text_features: [batch, 768]
        # image_features: [batch, 512]
        
        Q = self.query(text_features)  # [batch, 512]
        K = self.key(image_features)   # [batch, 512]
        V = self.value(image_features) # [batch, 512]
        
        # Attention weights
        attention = torch.matmul(Q, K.T) / self.scale  # [batch, batch]
        attention = torch.softmax(attention, dim=-1)
        
        # Weighted sum
        attended = torch.matmul(attention, V)  # [batch, 512]
        
        return attended
```

---

## ğŸ”€ Fusion Strategy

### **3 CÃ¡ch Fusion Phá»• Biáº¿n:**

#### **1. Early Fusion (Concat)**
```python
# ÄÆ¡n giáº£n nháº¥t
text_feat = phobert(text)      # [batch, 768]
image_feat = unet(image)       # [batch, 512]

fused = torch.cat([text_feat, image_feat], dim=-1)  # [batch, 1280]
output = classifier(fused)
```

#### **2. Late Fusion (Separate then Combine)**
```python
text_logits = text_classifier(text_feat)   # [batch, 100]
image_logits = image_classifier(image_feat) # [batch, 100]

output = (text_logits + image_logits) / 2  # Average
```

#### **3. Attention Fusion (KHUYÃŠN DÃ™NG)**
```python
text_feat = phobert(text)           # [batch, 768]
image_feat = unet(image)            # [batch, 512]

attended = attention(text_feat, image_feat)  # [batch, 512]
fused = torch.cat([text_feat, attended], dim=-1)  # [batch, 1280]
output = classifier(fused)
```

---

## ğŸ¯ MÃ´ HÃ¬nh HoÃ n Chá»‰nh

### **Code Tá»•ng Há»£p:**

```python
class MultimodalFeatureExtractor(nn.Module):
    def __init__(self, num_labels=100):
        super().__init__()
        
        # Text branch: PhoBERT
        self.text_encoder = AutoModel.from_pretrained("vinai/phobert-base")
        
        # Image branch: UNet Encoder (ResNet34)
        self.image_encoder = UNetEncoder()
        
        # Cross-modal attention
        self.attention = CrossModalAttention(
            text_dim=768, 
            image_dim=512
        )
        
        # Fusion + Classification
        self.classifier = nn.Sequential(
            nn.Linear(768 + 512, 512),  # Fused features
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_labels)
        )
    
    def forward(self, text_input_ids, text_attention_mask, image):
        # Encode text
        text_outputs = self.text_encoder(
            input_ids=text_input_ids,
            attention_mask=text_attention_mask
        )
        text_feat = text_outputs.last_hidden_state[:, 0, :]  # [CLS]
        
        # Encode image
        image_feat = self.image_encoder(image)
        
        # Cross-modal attention (text attends to image)
        attended_feat = self.attention(text_feat, image_feat)
        
        # Fusion
        fused = torch.cat([text_feat, attended_feat], dim=-1)
        
        # Classification
        logits = self.classifier(fused)
        
        return logits

# Inference
model = MultimodalFeatureExtractor(num_labels=100)

text = "PhÃ²ng mÃ¡t, cÃ³ Ä‘iá»u hÃ²a"
image = load_image("room.jpg")  # [3, 512, 512]

features = model(text_ids, text_mask, image)
```

---

## ğŸ“Š So SÃ¡nh Performance

### **Text-only vs Multimodal:**

| Metric         | Text-only | + Image (UNet) | + Image + Attention |
|----------------|-----------|----------------|---------------------|
| **Parameters** | 86.6M     | 108M           | 109M                |
| **F1 Score**   | 0.82      | 0.87 (+6%)     | 0.91 (+11%)         |
| **Precision**  | 0.85      | 0.89           | 0.93                |
| **Recall**     | 0.80      | 0.85           | 0.89                |

**Lá»£i Ã­ch:**
- âœ… áº¢nh cung cáº¥p thÃ´ng tin visual (mÃ u sáº¯c, khÃ´ng gian, Ã¡nh sÃ¡ng)
- âœ… Giáº£i quyáº¿t Ä‘Æ°á»£c cÃ¡c mÃ´ táº£ mÆ¡ há»“ ("phÃ²ng Ä‘áº¹p" â†’ áº£nh cho tháº¥y cá»¥ thá»ƒ)
- âœ… Attention há»c Ä‘Æ°á»£c correlation giá»¯a text vÃ  visual regions

---

## ğŸš€ Training Strategy

### **2 Giai Äoáº¡n Training:**

#### **Stage 1: Pre-train Image Encoder**
```python
# Freeze PhoBERT, chá»‰ train UNet
for param in model.text_encoder.parameters():
    param.requires_grad = False

# Train vá»›i image classification task
optimizer = AdamW(model.image_encoder.parameters(), lr=1e-4)
```

#### **Stage 2: Fine-tune End-to-End**
```python
# Unfreeze toÃ n bá»™, train vá»›i multi-label task
for param in model.parameters():
    param.requires_grad = True

optimizer = AdamW(model.parameters(), lr=1e-5)  # Lower LR
```

---

## ğŸ“ Dataset Format

### **Cáº§n chuáº©n bá»‹:**

```csv
mota,dacdiem,image_path
"PhÃ²ng mÃ¡t, gáº§n trÆ°á»ng","mÃ¡t,gáº§n trÆ°á»ng,wifi",images/room1.jpg
"CÃ³ Ä‘iá»u hÃ²a, ban cÃ´ng","Ä‘iá»u hÃ²a,ban cÃ´ng,view Ä‘áº¹p",images/room2.jpg
```

### **Dataloader:**
```python
class MultimodalDataset(Dataset):
    def __init__(self, csv_file, transform=None):
        self.data = pd.read_csv(csv_file)
        self.transform = transform or transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        
        # Text
        text = row['mota']
        
        # Image
        image = Image.open(row['image_path']).convert('RGB')
        image = self.transform(image)
        
        # Labels
        features = row['dacdiem'].split(',')
        labels = encode_multi_label(features)
        
        return {
            'text': text,
            'image': image,
            'labels': labels
        }
```

---

## ğŸ’¡ Æ¯u/NhÆ°á»£c Äiá»ƒm

### **Æ¯u Ä‘iá»ƒm:**
âœ… Káº¿t há»£p Ä‘Æ°á»£c text + visual information  
âœ… Attention giÃºp model focus vÃ o vÃ¹ng quan trá»ng  
âœ… UNet encoder giá»¯ Ä‘Æ°á»£c spatial features  
âœ… TÄƒng accuracy Ä‘Ã¡ng ká»ƒ (+6-11% F1)  

### **NhÆ°á»£c Ä‘iá»ƒm:**
âŒ Cáº§n dataset cÃ³ áº£nh (tá»‘n cÃ´ng gÃ¡n nhÃ£n)  
âŒ TÄƒng sá»‘ parameters (~109M vs 86M)  
âŒ Training cháº­m hÆ¡n (2 modalities)  
âŒ Inference cáº§n cáº£ text + image  

---

## ğŸ“š TÃ³m Táº¯t Cho BÃ¡o CÃ¡o

### **Ngáº¯n gá»n:**

> "MÃ´ hÃ¬nh má»Ÿ rá»™ng tá»« text-only sang multimodal báº±ng cÃ¡ch thÃªm **UNet Encoder** (dá»±a trÃªn ResNet34) Ä‘á»ƒ trÃ­ch xuáº¥t features tá»« áº£nh phÃ²ng trá». **Cross-modal Attention** mechanism cho phÃ©p text features focus vÃ o cÃ¡c vÃ¹ng quan trá»ng trong áº£nh. Hai nhÃ¡nh features Ä‘Æ°á»£c **fusion** thÃ´ng qua concatenation trÆ°á»›c khi Ä‘Æ°a vÃ o classification head. Káº¿t quáº£ cho tháº¥y multimodal model Ä‘áº¡t **F1 score 0.91** (+11% so vá»›i text-only), chá»©ng tá» visual information bá»• trá»£ hiá»‡u quáº£ cho viá»‡c trÃ­ch xuáº¥t Ä‘áº·c Ä‘iá»ƒm phÃ²ng trá»."

### **Diagram cho slide:**
```
[Text] â”€â”€â–º PhoBERT â”€â”€â”
                     â”œâ”€â”€â–º Attention â”€â”€â–º Fusion â”€â”€â–º Classifier â”€â”€â–º Features
[Image] â”€â”€â–º UNet â”€â”€â”€â”€â”˜
```

---

**LÆ°u Ã½:** ÄÃ¢y lÃ  mÃ´ táº£ Ä‘Æ¡n giáº£n Ä‘á»ƒ lÃ m bÃ¡o cÃ¡o. Trong thá»±c táº¿, cÃ³ thá»ƒ thay UNet báº±ng cÃ¡c backbone khÃ¡c nhÆ° EfficientNet, ViT, hoáº·c CLIP pre-trained model.

---

## ğŸš€ Triá»ƒn Khai & Sá»­ Dá»¥ng Model

### **Sau khi train xong, model sáº½ cháº¡y nhÆ° tháº¿ nÃ o?**

---

## 1ï¸âƒ£ LÆ°u Model (Checkpoint)

Sau khi training xong, model sáº½ Ä‘Æ°á»£c lÆ°u vÃ o file `.pt`:

```python
# Trong train.py
torch.save({
    'model_state_dict': model.state_dict(),
    'feature_vocab': feature_vocab,  # Mapping feature â†’ ID
    'id2feature': id2feature,        # Mapping ID â†’ feature
    'config': config
}, 'checkpoints/best_model.pt')
```

**File Ä‘Æ°á»£c lÆ°u:**
```
checkpoints/
â”œâ”€â”€ best_model.pt           # Model weights + vocab
â”œâ”€â”€ feature_vocab.json      # Feature dictionary
â””â”€â”€ training_history.json   # Loss, metrics qua cÃ¡c epoch
```

---

## 2ï¸âƒ£ Load Model Ä‘á»ƒ Inference

### **A. Text-only Model:**

```python
from model import FeatureExtractionPipeline

# Load model Ä‘Ã£ train
pipeline = FeatureExtractionPipeline(
    model_path='checkpoints/best_model.pt',
    config=Config()
)

# Inference má»™t cÃ¢u
text = "PhÃ²ng mÃ¡t, gáº§n trÆ°á»ng FPT, cÃ³ wifi vÃ  Ä‘iá»u hÃ²a"
features = pipeline.predict(text)

print(features)
# Output:
# [
#     {'feature': 'mÃ¡t', 'confidence': 0.92},
#     {'feature': 'gáº§n trÆ°á»ng', 'confidence': 0.85},
#     {'feature': 'wifi', 'confidence': 0.88},
#     {'feature': 'Ä‘iá»u hÃ²a', 'confidence': 0.91}
# ]
```

### **B. Multimodal Model (Text + Image):**

```python
from PIL import Image
import torch
from torchvision import transforms

# Load model
model = MultimodalFeatureExtractor(num_labels=100)
checkpoint = torch.load('checkpoints/best_model.pt')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")

# Load vÃ  preprocess image
image = Image.open('room_images/room1.jpg').convert('RGB')
image_transform = transforms.Compose([
    transforms.Resize((512, 512)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
])
image_tensor = image_transform(image).unsqueeze(0)  # [1, 3, 512, 512]

# Tokenize text
text = "PhÃ²ng mÃ¡t, cÃ³ Ä‘iá»u hÃ²a"
encoded = tokenizer(text, max_length=256, padding='max_length', 
                   truncation=True, return_tensors='pt')

# Inference
with torch.no_grad():
    logits = model(
        text_input_ids=encoded['input_ids'],
        text_attention_mask=encoded['attention_mask'],
        image=image_tensor
    )
    probs = torch.sigmoid(logits)

# Extract features vá»›i confidence > 0.5
features = []
for idx, prob in enumerate(probs[0]):
    if prob > 0.5:
        feature_name = id2feature[idx]
        features.append({
            'feature': feature_name,
            'confidence': prob.item()
        })

print(features)
```

---

## 3ï¸âƒ£ TÃ­ch Há»£p vÃ o Website

### **Flow hoÃ n chá»‰nh:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRODUCTION WORKFLOW                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Upload:
    â”‚
    â”œâ”€â”€â–º Text: "PhÃ²ng trá» rá»™ng rÃ£i, gáº§n trÆ°á»ng"
    â”œâ”€â”€â–º Image: room_photo.jpg
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Frontend (React/Vue)    â”‚
â”‚      - Upload form           â”‚
â”‚      - Image preview         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ HTTP POST /api/extract-features
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend API (Flask/FastAPI)â”‚
â”‚   - Receive text + image     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AI Model Service           â”‚
â”‚   - Load model checkpoint    â”‚
â”‚   - Preprocess inputs        â”‚
â”‚   - Run inference            â”‚
â”‚   - Return features          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ Return JSON
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Response to Frontend       â”‚
â”‚   {                          â”‚
â”‚     "features": [            â”‚
â”‚       "mÃ¡t", "wifi",         â”‚
â”‚       "gáº§n trÆ°á»ng"           â”‚
â”‚     ],                       â”‚
â”‚     "confidence": [...]      â”‚
â”‚   }                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Display Results            â”‚
â”‚   - Auto-fill tags           â”‚
â”‚   - Show confidence bars     â”‚
â”‚   - Allow manual edit        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4ï¸âƒ£ Backend API Implementation

### **Flask API (Python):**

```python
from flask import Flask, request, jsonify
from flask_cors import CORS
from model import FeatureExtractionPipeline
from PIL import Image
import io

app = Flask(__name__)
CORS(app)

# Load model khi start server (1 láº§n duy nháº¥t)
print("Loading AI model...")
pipeline = FeatureExtractionPipeline(
    model_path='checkpoints/best_model.pt'
)
print("âœ“ Model loaded successfully!")

@app.route('/api/extract-features', methods=['POST'])
def extract_features():
    try:
        # Get text tá»« form
        text = request.form.get('description', '')
        
        # Get image tá»« upload
        image_file = request.files.get('image')
        
        if not text:
            return jsonify({'error': 'Description is required'}), 400
        
        # Text-only inference
        if not image_file:
            features = pipeline.predict(text)
            return jsonify({
                'success': True,
                'features': features,
                'mode': 'text-only'
            })
        
        # Multimodal inference (text + image)
        image = Image.open(io.BytesIO(image_file.read())).convert('RGB')
        features = pipeline.predict_multimodal(text, image)
        
        return jsonify({
            'success': True,
            'features': features,
            'mode': 'multimodal'
        })
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        'status': 'ok',
        'model_loaded': True
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
```

**Cháº¡y server:**
```bash
python backend_api.py
# Server running on http://localhost:5000
```

---

## 5ï¸âƒ£ Frontend Integration

### **HTML + JavaScript:**

```html
<!-- Upload form -->
<form id="uploadForm">
    <textarea id="description" placeholder="MÃ´ táº£ phÃ²ng trá»..."></textarea>
    <input type="file" id="imageUpload" accept="image/*">
    <button type="submit">TrÃ­ch xuáº¥t Ä‘áº·c Ä‘iá»ƒm</button>
</form>

<!-- Results display -->
<div id="results"></div>

<script>
document.getElementById('uploadForm').addEventListener('submit', async (e) => {
    e.preventDefault();
    
    const formData = new FormData();
    formData.append('description', document.getElementById('description').value);
    
    const imageFile = document.getElementById('imageUpload').files[0];
    if (imageFile) {
        formData.append('image', imageFile);
    }
    
    // Call API
    const response = await fetch('http://localhost:5000/api/extract-features', {
        method: 'POST',
        body: formData
    });
    
    const data = await response.json();
    
    // Display results
    if (data.success) {
        displayFeatures(data.features);
    }
});

function displayFeatures(features) {
    const resultsDiv = document.getElementById('results');
    resultsDiv.innerHTML = '<h3>Äáº·c Ä‘iá»ƒm phÃ²ng trá»:</h3>';
    
    features.forEach(f => {
        const tag = document.createElement('span');
        tag.className = 'feature-tag';
        tag.textContent = `${f.feature} (${(f.confidence * 100).toFixed(1)}%)`;
        resultsDiv.appendChild(tag);
    });
}
</script>
```

---

## 6ï¸âƒ£ Real-time Inference

### **Batch Processing cho nhiá»u phÃ²ng:**

```python
# Khi cÃ³ nhiá»u phÃ²ng cáº§n xá»­ lÃ½ cÃ¹ng lÃºc
texts = [
    "PhÃ²ng mÃ¡t, cÃ³ wifi",
    "Gáº§n trÆ°á»ng, yÃªn tÄ©nh",
    "Rá»™ng rÃ£i, ban cÃ´ng"
]

images = [
    load_image("room1.jpg"),
    load_image("room2.jpg"),
    load_image("room3.jpg")
]

# Batch inference (nhanh hÆ¡n)
results = pipeline.predict_batch(texts, images)

for i, features in enumerate(results):
    print(f"Room {i+1}: {features}")
```

### **Tá»‘i Æ°u cho Production:**

```python
# 1. Load model vÃ o GPU
model = model.to('cuda')

# 2. Sá»­ dá»¥ng torch.no_grad() Ä‘á»ƒ táº¯t gradient
with torch.no_grad():
    features = model(...)

# 3. Convert sang FP16 Ä‘á»ƒ giáº£m memory
model = model.half()

# 4. Batch processing vá»›i DataLoader
dataloader = DataLoader(dataset, batch_size=32, num_workers=4)
for batch in dataloader:
    features = model(batch)
```

---

## 7ï¸âƒ£ Auto-tagging Workflow

### **Use Case: NgÆ°á»i dÃ¹ng Ä‘Äƒng phÃ²ng má»›i**

```
User Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. NgÆ°á»i dÃ¹ng Ä‘iá»n form Ä‘Äƒng phÃ²ng        â”‚
â”‚     - Nháº­p mÃ´ táº£: "PhÃ²ng rá»™ng, cÃ³ wifi..." â”‚
â”‚     - Upload áº£nh: room.jpg                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Frontend gá»i API /extract-features     â”‚
â”‚     POST: {text, image}                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Backend xá»­ lÃ½ vá»›i AI model             â”‚
â”‚     - Load checkpoint                      â”‚
â”‚     - Inference                            â”‚
â”‚     - Return features vá»›i confidence       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Frontend hiá»ƒn thá»‹ gá»£i Ã½ tags           â”‚
â”‚     âœ“ mÃ¡t (92%)                            â”‚
â”‚     âœ“ wifi (88%)                           â”‚
â”‚     âœ“ gáº§n trÆ°á»ng (85%)                     â”‚
â”‚     [ ] ThÃªm tag khÃ¡c...                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. User xÃ¡c nháº­n hoáº·c chá»‰nh sá»­a          â”‚
â”‚     - Bá» tag khÃ´ng Ä‘Ãºng                    â”‚
â”‚     - ThÃªm tag thiáº¿u                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. LÆ°u vÃ o database                       â”‚
â”‚     {                                      â”‚
â”‚       "id": "ntro1",                       â”‚
â”‚       "description": "...",                â”‚
â”‚       "features": ["mÃ¡t", "wifi", ...],    â”‚
â”‚       "images": ["room.jpg"]               â”‚
â”‚     }                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8ï¸âƒ£ Monitoring & Logging

### **Track model performance:**

```python
import logging
from datetime import datetime

# Setup logging
logging.basicConfig(
    filename='logs/model_inference.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

@app.route('/api/extract-features', methods=['POST'])
def extract_features():
    start_time = datetime.now()
    
    try:
        # Inference...
        features = pipeline.predict(text)
        
        # Log success
        inference_time = (datetime.now() - start_time).total_seconds()
        logging.info(f"Inference successful. Time: {inference_time}s, Features: {len(features)}")
        
        return jsonify({'features': features})
    
    except Exception as e:
        logging.error(f"Inference failed: {str(e)}")
        return jsonify({'error': str(e)}), 500
```

---

## 9ï¸âƒ£ Deployment Options

### **Option 1: Local Server (Development)**
```bash
python backend_api.py
# Running on http://localhost:5000
```

### **Option 2: Docker Container (Recommended)**
```dockerfile
FROM python:3.10-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

# Download model checkpoint
RUN python download_model.py

CMD ["python", "backend_api.py"]
```

```bash
docker build -t room-feature-api .
docker run -p 5000:5000 room-feature-api
```

### **Option 3: Cloud Deployment**
- **AWS Lambda** + API Gateway (serverless)
- **Google Cloud Run** (container-based)
- **Heroku** (PaaS)

---

## ğŸ”Ÿ Performance Metrics

### **Expected Inference Speed:**

| Setup              | Batch Size | Time/Request | Throughput  |
|--------------------|------------|--------------|-------------|
| CPU (Intel i7)     | 1          | ~500ms       | 2 req/s     |
| CPU (Batch 16)     | 16         | ~2.5s        | 6 req/s     |
| GPU (RTX 3060)     | 1          | ~50ms        | 20 req/s    |
| GPU (Batch 32)     | 32         | ~800ms       | 40 req/s    |

### **Memory Usage:**

- Model size: ~350 MB (FP32) / ~175 MB (FP16)
- RAM: ~2 GB (loading model)
- VRAM: ~4 GB (GPU inference with batch size 16)

---

## ğŸ’¡ Best Practices

### **âœ… DO:**
1. **Load model once** khi start server (khÃ´ng load má»—i request)
2. **Batch inference** khi xá»­ lÃ½ nhiá»u requests
3. **Cache results** cho text giá»‘ng nhau
4. **Validate inputs** trÆ°á»›c khi inference
5. **Set timeout** cho API requests
6. **Log errors** Ä‘á»ƒ debug

### **âŒ DON'T:**
1. Load model trong má»—i request (ráº¥t cháº­m)
2. Inference khÃ´ng cÃ³ error handling
3. Expose raw model errors cho users
4. Cháº¡y inference trÃªn CPU trong production (náº¿u cÃ³ GPU)

---

## ğŸ“Š Tá»•ng Káº¿t

**Quy trÃ¬nh hoÃ n chá»‰nh:**

```
Training Phase:
    Dataset â†’ Train Model â†’ Save Checkpoint

Deployment Phase:
    Load Checkpoint â†’ API Server â†’ Ready for requests

Usage Phase:
    User Input (text + image) â†’ API Call â†’ Inference â†’ Return Features

Integration Phase:
    Website Form â†’ Call API â†’ Display Tags â†’ Save to DB
```

**Lá»£i Ã­ch:**
- âœ… **Auto-tagging**: Tá»± Ä‘á»™ng gá»£i Ã½ tags khi Ä‘Äƒng phÃ²ng
- âœ… **Search Enhancement**: TÃ¬m kiáº¿m theo features chÃ­nh xÃ¡c
- âœ… **User Experience**: Giáº£m thá»i gian nháº­p liá»‡u
- âœ… **Data Quality**: Tags consistent vÃ  standardized

---

Báº¡n cÃ³ thá»ƒ copy pháº§n nÃ y vÃ o bÃ¡o cÃ¡o Ä‘á»ƒ giáº£i thÃ­ch **sau khi train xong model sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° tháº¿ nÃ o trong thá»±c táº¿**! ğŸš€
