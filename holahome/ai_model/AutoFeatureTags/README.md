# Feature Extraction Model for Room Descriptions

Há»‡ thá»‘ng trÃ­ch xuáº¥t tá»± Ä‘á»™ng cÃ¡c Ä‘áº·c Ä‘iá»ƒm tá»« mÃ´ táº£ phÃ²ng trá» sá»­ dá»¥ng Deep Learning (PhoBERT).

## ğŸ—ï¸ Kiáº¿n trÃºc Pipeline

```
[Raw mÃ´ táº£ trá»]
        â”‚
        â–¼
[Tiá»n xá»­ lÃ½ + Tokenizer (PhoBERT)]
        â”‚
        â–¼
[Encoder (PhoBERT - Vietnamese BERT)]
        â”‚
        â–¼
[Classification Head (Multi-label)]
        â”‚
        â–¼
[Output: Danh sÃ¡ch Ä‘áº·c Ä‘iá»ƒm + confidence scores]
```

## ğŸ“‹ YÃªu cáº§u

```bash
pip install torch transformers pandas scikit-learn tqdm numpy
```

Hoáº·c cÃ i Ä‘áº·t tá»« file:
```bash
pip install -r requirements.txt
```

## ğŸ“Š Äá»‹nh dáº¡ng Dataset

Dataset cáº§n á»Ÿ dáº¡ng CSV vá»›i 2 cá»™t:

| description | features |
|-------------|----------|
| TÃ´i tháº¥y phÃ²ng trá» nÃ y khÃ¡ lÃ  mÃ¡t, gáº§n tiá»‡m táº¡p hÃ³a. PhÃ²ng cÃ³ bÃ¬nh nÃ³ng láº¡nh Ä‘áº§y Ä‘á»§, bÃ n há»c vÃ  bÃ n gháº¿ | mÃ¡t, gáº§n táº¡p hÃ³a, bÃ¬nh nÃ³ng láº¡nh, bÃ n há»c, gháº¿ |
| PhÃ²ng trá» rá»™ng rÃ£i, cÃ³ Ä‘iá»u hÃ²a, wifi miá»…n phÃ­. Gáº§n chá»£ vÃ  trÆ°á»ng há»c | rá»™ng rÃ£i, Ä‘iá»u hÃ²a, wifi miá»…n phÃ­, gáº§n chá»£, gáº§n trÆ°á»ng |

- **Cá»™t 1 (description)**: VÄƒn báº£n mÃ´ táº£ phÃ²ng trá»
- **Cá»™t 2 (features)**: CÃ¡c Ä‘áº·c Ä‘iá»ƒm cáº§n trÃ­ch xuáº¥t, phÃ¢n cÃ¡ch bá»Ÿi dáº¥u pháº©y

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Chuáº©n bá»‹ dá»¯ liá»‡u

Táº¡o dá»¯ liá»‡u máº«u vÃ  chia train/val/test:

```bash
python prepare_data.py
```

Hoáº·c tá»± chuáº©n bá»‹ file CSV vÃ  Ä‘áº·t vÃ o thÆ° má»¥c `data/`:
- `data/train.csv`
- `data/val.csv`
- `data/test.csv`

### 2. Huáº¥n luyá»‡n model

```bash
python train.py
```

Model sáº½ Ä‘Æ°á»£c lÆ°u táº¡i:
- `checkpoints/best_model.pt` - Model tá»‘t nháº¥t (theo F1 score)
- `checkpoints/checkpoint_epoch_X.pt` - Checkpoint má»—i epoch
- `checkpoints/feature_vocab.json` - Tá»« Ä‘iá»ƒn features

### 3. Dá»± Ä‘oÃ¡n vá»›i model Ä‘Ã£ train

**Dá»± Ä‘oÃ¡n 1 cÃ¢u:**
```bash
python inference.py --text "PhÃ²ng trá» mÃ¡t máº», cÃ³ Ä‘iá»u hÃ²a vÃ  wifi"
```

**Cháº¡y demo vá»›i nhiá»u cÃ¢u:**
```bash
python inference.py
```

**Sá»­ dá»¥ng model khÃ¡c:**
```bash
python inference.py --model checkpoints/checkpoint_epoch_10.pt --text "PhÃ²ng cÃ³ ban cÃ´ng rá»™ng"
```

### 4. Sá»­ dá»¥ng trong code Python

```python
from model import FeatureExtractionPipeline

# Load model
pipeline = FeatureExtractionPipeline(model_path='checkpoints/best_model.pt')

# Predict single text
text = "PhÃ²ng trá» gáº§n trÆ°á»ng, cÃ³ mÃ¡y láº¡nh vÃ  wifi"
features = pipeline.predict(text)

for feat in features:
    print(f"{feat['feature']}: {feat['confidence']:.2f}")

# Predict batch
texts = [
    "PhÃ²ng rá»™ng, cÃ³ giÆ°á»ng tá»§",
    "Gáº§n chá»£, an ninh tá»‘t"
]
results = pipeline.predict_batch(texts)
```

## âš™ï¸ Cáº¥u hÃ¬nh

Chá»‰nh sá»­a file `config.py` Ä‘á»ƒ thay Ä‘á»•i cÃ¡c tham sá»‘:

```python
class Config:
    # Model
    MODEL_NAME = "vinai/phobert-base"  # Pre-trained model
    MAX_LENGTH = 256                    # Max sequence length
    NUM_LABELS = 100                    # Max number of features
    
    # Training
    BATCH_SIZE = 16
    LEARNING_RATE = 2e-5
    NUM_EPOCHS = 10
    
    # Prediction
    MIN_FEATURE_CONFIDENCE = 0.5  # Threshold for feature extraction
```

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
AutoFeatureTags/
â”œâ”€â”€ config.py           # Cáº¥u hÃ¬nh model vÃ  training
â”œâ”€â”€ model.py            # Äá»‹nh nghÄ©a model architecture
â”œâ”€â”€ train.py            # Script huáº¥n luyá»‡n
â”œâ”€â”€ inference.py        # Script dá»± Ä‘oÃ¡n
â”œâ”€â”€ prepare_data.py     # Script chuáº©n bá»‹ dá»¯ liá»‡u
â”œâ”€â”€ README.md           # File nÃ y
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ data/               # ThÆ° má»¥c chá»©a dataset
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ val.csv
â”‚   â””â”€â”€ test.csv
â”œâ”€â”€ checkpoints/        # ThÆ° má»¥c lÆ°u model
â”‚   â”œâ”€â”€ best_model.pt
â”‚   â””â”€â”€ feature_vocab.json
â””â”€â”€ logs/               # ThÆ° má»¥c lÆ°u training logs
    â””â”€â”€ training_history.json
```

## ğŸ¯ Káº¿t quáº£ Training

Training sáº½ hiá»ƒn thá»‹ cÃ¡c metrics:
- **Loss**: Training vÃ  Validation loss
- **F1 Score** (micro & macro): ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng trÃ­ch xuáº¥t
- **Precision**: Äá»™ chÃ­nh xÃ¡c cá»§a features Ä‘Æ°á»£c trÃ­ch xuáº¥t
- **Recall**: Tá»· lá»‡ features Ä‘Æ°á»£c phÃ¡t hiá»‡n

Example output:
```
==================================================
Epoch 5/10
==================================================
Training Loss: 0.0234
Validation Loss: 0.0312
F1 (micro): 0.8756
F1 (macro): 0.8234
Precision: 0.9012
Recall: 0.8523
âœ“ New best model saved! F1: 0.8756
```

## ğŸ”§ Troubleshooting

**1. Out of memory error:**
- Giáº£m `BATCH_SIZE` trong `config.py`
- Giáº£m `MAX_LENGTH`

**2. Model khÃ´ng há»c Ä‘Æ°á»£c:**
- TÄƒng `NUM_EPOCHS`
- Thá»­ `LEARNING_RATE` khÃ¡c (1e-5 Ä‘áº¿n 5e-5)
- Kiá»ƒm tra data cÃ³ Ä‘Ãºng format khÃ´ng

**3. KhÃ´ng cÃ³ GPU:**
- Äá»•i `DEVICE = "cpu"` trong `config.py`
- Training sáº½ cháº­m hÆ¡n nhÆ°ng váº«n hoáº¡t Ä‘á»™ng

## ğŸ“ VÃ­ dá»¥ Input/Output

**Input:**
```
"TÃ´i tháº¥y phÃ²ng trá» nÃ y khÃ¡ lÃ  mÃ¡t, gáº§n tiá»‡m táº¡p hÃ³a. 
PhÃ²ng cÃ³ bÃ¬nh nÃ³ng láº¡nh Ä‘áº§y Ä‘á»§, bÃ n há»c vÃ  bÃ n gháº¿"
```

**Output:**
```python
[
    {'feature': 'mÃ¡t', 'confidence': 0.92},
    {'feature': 'gáº§n táº¡p hÃ³a', 'confidence': 0.87},
    {'feature': 'bÃ¬nh nÃ³ng láº¡nh', 'confidence': 0.95},
    {'feature': 'bÃ n há»c', 'confidence': 0.89},
    {'feature': 'gháº¿', 'confidence': 0.85}
]
```

## ğŸ“ˆ Má»Ÿ rá»™ng

1. **ThÃªm dá»¯ liá»‡u training**: CÃ ng nhiá»u data, model cÃ ng chÃ­nh xÃ¡c
2. **Fine-tune hyperparameters**: Thá»­ nghiá»‡m vá»›i learning rate, batch size
3. **Thá»­ model khÃ¡c**: CÃ³ thá»ƒ thay PhoBERT báº±ng XLM-RoBERTa hoáº·c mBERT
4. **ThÃªm post-processing**: Lá»c features trÃ¹ng láº·p, group theo category

## ğŸ“ Há»— trá»£

Náº¿u gáº·p váº¥n Ä‘á», hÃ£y kiá»ƒm tra:
1. Data format Ä‘Ãºng chÆ°a
2. Dependencies Ä‘Ã£ cÃ i Ä‘á»§ chÆ°a
3. Model path cÃ³ Ä‘Ãºng khÃ´ng
