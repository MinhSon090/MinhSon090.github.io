# ğŸ—ï¸ Kiáº¿n TrÃºc MÃ´ HÃ¬nh Feature Extraction

## ğŸ“Š Tá»•ng Quan Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEATURE EXTRACTION PIPELINE                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input Text (Vietnamese)
    â”‚
    â”œâ”€â”€â–º "TÃ´i tháº¥y phÃ²ng trá» nÃ y khÃ¡ lÃ  mÃ¡t, gáº§n tiá»‡m táº¡p hÃ³a.
    â”‚     PhÃ²ng cÃ³ bÃ¬nh nÃ³ng láº¡nh Ä‘áº§y Ä‘á»§, bÃ n há»c vÃ  bÃ n gháº¿"
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TOKENIZER      â”‚ â† PhoBERT Tokenizer (Vietnamese)
â”‚  (vinai/phobert) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€â–º Token IDs: [0, 245, 1523, 789, ...]
    â”œâ”€â”€â–º Attention Mask: [1, 1, 1, 0, 0, ...]
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BERT ENCODER    â”‚ â† 12 Transformer Layers
â”‚  (PhoBERT-base)  â”‚   768-dim embeddings
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€â–º Contextualized Embeddings
    â”‚     Shape: [batch, seq_len, 768]
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  POOLING LAYER   â”‚ â† Extract [CLS] token
â”‚  [CLS] token     â”‚   (first token representation)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€â–º Pooled Output
    â”‚     Shape: [batch, 768]
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    DROPOUT       â”‚ â† Dropout (p=0.1)
â”‚    (p=0.1)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLASSIFICATION  â”‚ â† Dense (768 â†’ 768)
â”‚      HEAD        â”‚   ReLU Activation
â”‚   (MLP 2 layers) â”‚   Dropout (p=0.1)
â”‚                  â”‚   Dense (768 â†’ 100)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€â–º Logits
    â”‚     Shape: [batch, 100]
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    SIGMOID       â”‚ â† Multi-label Classification
â”‚   (Threshold)    â”‚   Confidence > 0.5
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Output Features
    â”‚
    â”œâ”€â”€â–º ["mÃ¡t", "gáº§n táº¡p hÃ³a", "bÃ¬nh nÃ³ng láº¡nh", 
    â”‚     "bÃ n há»c", "gháº¿"]
```

---

## ğŸ›ï¸ Kiáº¿n TrÃºc Chi Tiáº¿t

### 1ï¸âƒ£ **Input Layer - Tokenization**

**ThÃ nh pháº§n:**
- **PhoBERT Tokenizer** (vinai/phobert-base)
- Vocabulary size: ~64,000 tokens
- Supports Vietnamese word segmentation

**Xá»­ lÃ½:**
```python
Input: "PhÃ²ng cÃ³ bÃ¬nh nÃ³ng láº¡nh"
â†“
Token IDs: [0, 245, 34, 1523, 789, 234, 2]
Attention Mask: [1, 1, 1, 1, 1, 1, 1]
Position IDs: [0, 1, 2, 3, 4, 5, 6]
```

**Hyperparameters:**
- `MAX_LENGTH = 256` - Cáº¯t/pad Ä‘áº¿n 256 tokens
- `padding = 'max_length'` - Pad vá»›i token [PAD]
- `truncation = True` - Cáº¯t náº¿u vÆ°á»£t quÃ¡

---

### 2ï¸âƒ£ **Encoder - PhoBERT**

**Kiáº¿n trÃºc:**
```
PhoBERT-base (BERT for Vietnamese)
â”œâ”€â”€ 12 Transformer Encoder Layers
â”‚   â”œâ”€â”€ Multi-Head Self-Attention (12 heads)
â”‚   â”‚   â””â”€â”€ head_dim = 768 / 12 = 64
â”‚   â”œâ”€â”€ Feed-Forward Network (768 â†’ 3072 â†’ 768)
â”‚   â””â”€â”€ Layer Normalization + Residual Connections
â”‚
â”œâ”€â”€ Embedding Layer
â”‚   â”œâ”€â”€ Token Embeddings (64k vocab â†’ 768 dim)
â”‚   â”œâ”€â”€ Position Embeddings (512 positions â†’ 768 dim)
â”‚   â””â”€â”€ Segment Embeddings (2 segments â†’ 768 dim)
â”‚
â””â”€â”€ Parameters: ~86 million
```

**Output:**
- **last_hidden_state**: [batch, seq_len, 768]
  - Contextualized embeddings cho má»—i token
  - Token Ä‘áº§u tiÃªn `[CLS]` chá»©a representation cá»§a toÃ n bá»™ cÃ¢u

**VÃ­ dá»¥:**
```python
Input tokens: [CLS] PhÃ²ng mÃ¡t gáº§n trÆ°á»ng [SEP]
              â†“     â†“    â†“   â†“     â†“     â†“
Embeddings:  e0    e1   e2  e3    e4    e5
              â†“ (Self-Attention Ã— 12 layers)
Contextualized: 
              h0    h1   h2  h3    h4    h5
              â†‘
              [CLS] token = Sentence representation
```

---

### 3ï¸âƒ£ **Pooling Layer**

**PhÆ°Æ¡ng phÃ¡p:** Extract [CLS] token

```python
outputs = encoder(input_ids, attention_mask)
pooled = outputs.last_hidden_state[:, 0, :]  # Láº¥y token Ä‘áº§u tiÃªn
```

**Táº¡i sao dÃ¹ng [CLS]?**
- BERT Ä‘Æ°á»£c pre-train Ä‘á»ƒ [CLS] token chá»©a context cá»§a toÃ n bá»™ cÃ¢u
- Suitable cho sentence-level classification tasks
- Shape: `[batch, 768]`

---

### 4ï¸âƒ£ **Classification Head (Decoder)**

**Kiáº¿n trÃºc:**

```
Input: [batch, 768]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Dropout (0.1)    â”‚ â† Regularization
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Linear(768 â†’ 768)  â”‚ â† Hidden layer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ReLU()         â”‚ â† Non-linearity
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Dropout (0.1)    â”‚ â† Regularization
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Linear(768 â†’ 100)  â”‚ â† Output layer (100 features)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Output: [batch, 100] logits
```

**Parameters:**
```
Layer 1: 768 Ã— 768 + 768 = 590,592 params
Layer 2: 768 Ã— 100 + 100 = 76,900 params
Total: ~667K params
```

---

### 5ï¸âƒ£ **Output & Loss**

**Multi-Label Classification:**

```python
# Forward pass
logits = model(input_ids, attention_mask)  # [batch, 100]

# Apply sigmoid (not softmax!)
probs = torch.sigmoid(logits)  # [batch, 100]

# Each output is independent (0-1)
# Example: [0.12, 0.89, 0.67, 0.03, ...]
```

**Loss Function:**
```python
criterion = nn.BCEWithLogitsLoss()
loss = criterion(logits, labels)
```

**BCE Loss cho Multi-Label:**
- Má»—i label Ä‘á»™c láº­p (khÃ´ng loáº¡i trá»« láº«n nhau)
- Má»™t sample cÃ³ thá»ƒ cÃ³ nhiá»u labels = 1
- Loss = -Î£[y*log(Ïƒ(x)) + (1-y)*log(1-Ïƒ(x))]

---

## ğŸ“ˆ Training Pipeline

### Data Flow:

```
dataset.csv (10,000 rows)
    â”‚
    â”œâ”€â”€â–º Column 1: mota (description text)
    â”œâ”€â”€â–º Column 2: dacdiem (features: "mÃ¡t, gáº§n chá»£, wifi")
    â”‚
    â–¼
split_dataset.py
    â”‚
    â”œâ”€â”€â–º train.csv (8,000 rows - 80%)
    â””â”€â”€â–º val.csv (2,000 rows - 20%)
    â”‚
    â–¼
FeatureDataset.__getitem__()
    â”‚
    â”œâ”€â”€â–º Tokenize text
    â”œâ”€â”€â–º Parse features â†’ multi-hot vector [0,1,0,1,...]
    â”‚
    â–¼
DataLoader (batch_size=16)
    â”‚
    â–¼
Training Loop
    â”‚
    â”œâ”€â”€â–º Forward Pass
    â”œâ”€â”€â–º BCEWithLogitsLoss
    â”œâ”€â”€â–º Backward Pass
    â”œâ”€â”€â–º Optimizer.step()
    â”‚
    â–¼
Validation
    â”‚
    â”œâ”€â”€â–º F1 Score
    â”œâ”€â”€â–º Precision
    â”œâ”€â”€â–º Recall
    â”‚
    â–¼
Save Best Model (best F1)
```

---

## ğŸ”¢ Model Capacity

### Total Parameters:

```
Component                Parameters
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PhoBERT Encoder         ~86,000,000
Classification Head        667,492
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total                   ~86,667,492
```

### Memory Requirements:

```
Model Size (fp32):     ~347 MB
Model Size (fp16):     ~173 MB
Inference (batch=1):   ~2 GB GPU
Training (batch=16):   ~8 GB GPU
```

---

## âš™ï¸ Hyperparameters

### Model Config:
```python
MODEL_NAME = "vinai/phobert-base"
MAX_LENGTH = 256              # Max tokens per input
HIDDEN_SIZE = 768             # BERT embedding dimension
NUM_LABELS = 100              # Number of feature types
DROPOUT = 0.1                 # Dropout probability
```

### Training Config:
```python
BATCH_SIZE = 16               # Samples per batch
LEARNING_RATE = 2e-5          # AdamW learning rate
NUM_EPOCHS = 10               # Training epochs
WARMUP_STEPS = 500            # Linear warmup steps
WEIGHT_DECAY = 0.01           # L2 regularization
MIN_FEATURE_CONFIDENCE = 0.5  # Threshold for predictions
```

### Optimizer:
```python
optimizer = AdamW(
    model.parameters(),
    lr=2e-5,
    weight_decay=0.01
)

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=500,
    num_training_steps=total_steps
)
```

---

## ğŸ¯ Inference Pipeline

### Single Text Prediction:

```python
# 1. Load model
pipeline = FeatureExtractionPipeline(
    model_path='checkpoints/best_model.pt'
)

# 2. Input text
text = "PhÃ²ng cÃ³ Ä‘iá»u hÃ²a, wifi, gáº§n trÆ°á»ng"

# 3. Extract features
features = pipeline.predict(text)

# 4. Output
[
    {'feature': 'Ä‘iá»u hÃ²a', 'confidence': 0.92},
    {'feature': 'wifi', 'confidence': 0.87},
    {'feature': 'gáº§n trÆ°á»ng', 'confidence': 0.78}
]
```

### Batch Prediction:

```python
texts = [
    "PhÃ²ng rá»™ng, cÃ³ ban cÃ´ng",
    "GiÃ¡ ráº», gáº§n chá»£, sáº¡ch sáº½"
]

results = pipeline.predict_batch(texts)
```

---

## ğŸ“Š Evaluation Metrics

### Multi-Label Metrics:

```python
# Micro-averaged (overall)
F1_micro = 2 * (P_micro * R_micro) / (P_micro + R_micro)

# Macro-averaged (per label)
F1_macro = mean([F1_label1, F1_label2, ...])

# Precision
Precision = TP / (TP + FP)

# Recall
Recall = TP / (TP + FN)
```

### Example Output:
```
Epoch 10/10
Train Loss: 0.0234
Val Loss: 0.0456
F1 (micro): 0.8234
F1 (macro): 0.7891
Precision: 0.8512
Recall: 0.7967
```

---

## ğŸš€ Optimizations

### Potential Improvements:

1. **Data Augmentation**
   - Back-translation
   - Synonym replacement
   - Random deletion/insertion

2. **Model Architecture**
   - Try PhoBERT-large (110M params)
   - Add attention pooling instead of [CLS]
   - Multi-head classification

3. **Training Tricks**
   - Label smoothing
   - Focal loss for imbalanced labels
   - Gradient accumulation for larger batch size

4. **Post-processing**
   - Confidence threshold tuning
   - Rule-based filtering
   - Feature clustering

---

## ğŸ“ Project Structure

```
AutoFeatureTags/
â”œâ”€â”€ config.py           # Hyperparameters
â”œâ”€â”€ model.py            # Model architecture
â”œâ”€â”€ train.py            # Training script
â”œâ”€â”€ inference.py        # Inference script
â”œâ”€â”€ prepare_data.py     # Data preprocessing
â”œâ”€â”€ split_dataset.py    # 80/20 split
â”œâ”€â”€ requirements.txt    # Dependencies
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset.csv     # Original data (10k rows)
â”‚   â”œâ”€â”€ train.csv       # Training set (8k)
â”‚   â””â”€â”€ val.csv         # Validation set (2k)
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best_model.pt   # Best model checkpoint
â”‚   â”œâ”€â”€ feature_vocab.json  # Feature vocabulary
â”‚   â””â”€â”€ checkpoint_epoch_*.pt
â”‚
â””â”€â”€ logs/
    â””â”€â”€ training_history.json
```

---

## ğŸ“ References

1. **PhoBERT**: Pre-trained language models for Vietnamese
   - Paper: https://arxiv.org/abs/2003.00744
   - Model: vinai/phobert-base

2. **BERT**: Bidirectional Encoder Representations from Transformers
   - Paper: https://arxiv.org/abs/1810.04805

3. **Multi-Label Classification**
   - Sigmoid + BCE Loss for independent labels

---

**TÃ³m táº¯t:** MÃ´ hÃ¬nh sá»­ dá»¥ng PhoBERT (pre-trained Vietnamese BERT) lÃ m encoder Ä‘á»ƒ hiá»ƒu ngá»¯ nghÄ©a, káº¿t há»£p vá»›i classification head (MLP 2 layers) Ä‘á»ƒ dá»± Ä‘oÃ¡n multi-label features tá»« mÃ´ táº£ phÃ²ng trá». Training vá»›i 8k samples, validation vá»›i 2k samples, optimize báº±ng AdamW vá»›i linear warmup scheduler.
